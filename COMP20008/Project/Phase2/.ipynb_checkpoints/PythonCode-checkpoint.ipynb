{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-2-5e8b50350c48>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-5e8b50350c48>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    df = pd.read_csv(filename)\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "# necessary libraries for this module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import plotly.plotly as py\n",
    "\n",
    "# Create new dataframe\n",
    "def create_dataframe(filename):\n",
    "    \"\"\"\n",
    "    Creates a dataframe from filename specified.\n",
    "    Also strips any whitespace for columns headers.\n",
    "    \n",
    "    Args:\n",
    "        filename: a string specifying dataset in csv format\n",
    "    Returns:\n",
    "        a pandas dataframe corresponding to the dataset provided\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # strips whitespace\n",
    "    df = df.rename(columns = lambda x: x.strip())\n",
    "    return df\n",
    "\n",
    "def check_lgas(df1, df2):\n",
    "    \"\"\"\n",
    "    Some preprocessing checking in order to check if both dataframes\n",
    "    contain same amount of LGAs. \n",
    "    \n",
    "    Args:\n",
    "        df1: first dataframe corresponding to dataset 1\n",
    "        df2: second dataframe corresponding to dataset 2\n",
    "    Returns:\n",
    "        true if both dataframes match, false if not\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert lga codes into lists\n",
    "    l1 = list(df1['lga_code']) \n",
    "    l2 = list(df2['lga_code'])\n",
    "\n",
    "    if len(l1) != len(l2):\n",
    "        return False\n",
    "\n",
    "    # check if any values are not common in both dataframes\n",
    "    if len(list(set(l2) - set(l1))) != 0 or len(list(set(l1) - set(l2))) != 0:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def bar_chart_homeless_pop(df):\n",
    "    \"\"\"\n",
    "    A function used in phase 2 to show bar charts of the homeless populations\n",
    "    \n",
    "    Args:\n",
    "        df: the dataframe to be plotted\n",
    "    Returns:\n",
    "        bar chart of homeless\n",
    "    \"\"\"\n",
    "    \n",
    "    data = dict(zip(df['lga_code'], df['homeless_ppl_est_per_1000_pop']))\n",
    "    plt.bar(range(len(data)), data.values(), width=1/1.5)\n",
    "    plt.xticks(range(len(data)), data.keys(), rotation=90)\n",
    "    plt.title('LGA Homeless Population per 1000 Residents')\n",
    "    plt.xlabel('LGA Codes')\n",
    "    plt.ylabel('Homeless Population per 1000')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "# scatter plot between two columns\n",
    "def create_scatter(col1, col2):\n",
    "    \"\"\"\n",
    "    Creates scatter plot of two variables, col1 and col2.\n",
    "    Args:\n",
    "        col1: first variable\n",
    "        col2: second variable\n",
    "    Returns:\n",
    "        scatter plot of both variables\n",
    "    \"\"\"\n",
    "    \n",
    "    # put the columns into lists, easier to work with\n",
    "    x = [x for x in df1[col1]]\n",
    "    y = [y for y in df1[col2]]\n",
    "\n",
    "    plt.scatter(x, y)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def calculate_pearsons_corr(col1, col2):\n",
    "    \"\"\"\n",
    "    Calculates pearsons correlation of two variables.\n",
    "    Args:\n",
    "        col1: first variable\n",
    "        col2: second variable\n",
    "    Returns:\n",
    "        returns correlation coefficient\n",
    "    \"\"\"\n",
    "    return df1[col1].corr(df1[col2])\n",
    "\n",
    "# creating dataframes for datasets\n",
    "df1 = create_dataframe('dataset1.csv')\n",
    "df2 = create_dataframe('dataset2.csv')\n",
    "\n",
    "#reordering columns\n",
    "new_order_col = ['lga_name', 'lga_code', 'homeless_ppl_est_per_1000_pop', 'social_housing_dwellings', \n",
    "                 'drug_usage_and_possession_offences_per_1000_pop', 'rental_housing_that_is_affordable_perc', \n",
    "                 'ppl_born_overseas_perc', 'unemployment_rate_perc','ppl_who_did_not_complete_yr_12_perc', \n",
    "                 'ppl_who_are_def_able_to_get_help_from_nbrs_perc']\n",
    "\n",
    "df1 = df1[new_order_col]\n",
    "\n",
    "#Adding population to first dataframe\n",
    "df1['Population_2015'] = df2['tpop_2015']\n",
    "\n",
    "# new column names, since the above are too long and hard to index\n",
    "new_cols = ['lga_name', 'lga_code', 'homeless_count', 'social_housing_count', 'drug_offences', 'rental_housing',\n",
    "              'born_overseas', 'unemployment_rate', 'year_12_Education', 'support', 'Population_2015']\n",
    "\n",
    "# renaming columns. \n",
    "df1.columns = new_cols\n",
    "\n",
    "def scatterplots_to_screen(col1, col2):\n",
    "    \"\"\"\n",
    "    Outputs scatter plot of two varaibles, along with correlations\n",
    "    \n",
    "    Args:\n",
    "        col1: the first variable\n",
    "        col2: the second variable\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    create_scatter(col1, col2)\n",
    "    print(\"Correlation: \" + str(calculate_pearsons_corr(col1, col2)))\n",
    "    return\n",
    "\n",
    "# All the scatter plots of each feature\n",
    "# Since different titles and labels are used, this proccess is very repetitive.\n",
    "plt.title('Homelessness vs Social Housing per LGA')\n",
    "plt.xlabel('Homeless people per 1000 population')\n",
    "plt.ylabel('Social housing dwellings per LGA')\n",
    "scatterplots_to_screen('homeless_count', 'social_housing_count')\n",
    "\n",
    "plt.title('Homelessness vs Drug Usage per LGA')\n",
    "plt.xlabel('Homeless people per 1000 population')\n",
    "plt.ylabel('Drug Usage per LGA')\n",
    "scatterplots_to_screen('homeless_count', 'drug_offences')\n",
    "\n",
    "plt.title('Homelessness vs Rental Housing Affordability per LGA')\n",
    "plt.xlabel('Homeless people per 1000 population')\n",
    "plt.ylabel('Rental Housing percentages')\n",
    "scatterplots_to_screen('homeless_count', 'rental_housing')\n",
    "\n",
    "plt.title('Homelessness vs People born overseas per LGA')\n",
    "plt.xlabel('Homeless people per 1000 population')\n",
    "plt.ylabel('People born overseas percentage')\n",
    "scatterplots_to_screen('homeless_count', 'born_overseas')\n",
    "\n",
    "plt.title('Homelessness vs Unemployment rate percentage per LGA')\n",
    "plt.xlabel('Homeless people per 1000 population')\n",
    "plt.ylabel('Unemployment rate percentage')\n",
    "scatterplots_to_screen('homeless_count', 'unemployment_rate')\n",
    "\n",
    "plt.title('Homelessness vs People without Year 12 percentage per LGA')\n",
    "plt.xlabel('Homeless people per 1000 population')\n",
    "plt.ylabel('People who did not finish Year 12 percentage')\n",
    "scatterplots_to_screen('homeless_count', 'year_12_Education')\n",
    "\n",
    "plt.title('Homelessness vs People able to get social support per LGA')\n",
    "plt.xlabel('Homeless people per 1000 population')\n",
    "plt.ylabel('People able to get help from social support percentage')\n",
    "scatterplots_to_screen('homeless_count', 'support')\n",
    "\n",
    "plt.title('Homelessness vs Population per LGA')\n",
    "plt.xlabel('Homeless people per 1000 population')\n",
    "plt.ylabel('LGA Population 2015')\n",
    "scatterplots_to_screen('homeless_count', 'Population_2015')\n",
    "\n",
    "# correlation data used for matrix\n",
    "correlation_data = {'homeless_count':df1['homeless_count'], 'social_housing_count':df1['social_housing_count'], \n",
    "                    'drug_offences': df1['drug_offences'], 'rental_housing': df1['rental_housing'],\n",
    "                    'born_overseas': df1['born_overseas'], 'unemployment_rate': df1['unemployment_rate'], \n",
    "                    'year_12_Education': df1['year_12_Education'], 'support': df1['support'], \n",
    "                    'Population_2015': df1['Population_2015']}\n",
    "\n",
    "# converting correlation data to dataframe\n",
    "correlation_df = pd.DataFrame(data = correlation_data)\n",
    "\n",
    "\n",
    "def create_correlation_matrix(df, columns):\n",
    "    \"\"\"\n",
    "    Outputs correlation matrix of dataframe.\n",
    "    Code taken from http://machinelearningmastery.com/visualize-machine-learning-data-python-pandas/\n",
    "    \n",
    "    Args:\n",
    "        df: the dataframe to calculate correlation\n",
    "        columns: column headers to include in correlation\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    correlations = df.corr()\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "    fig.colorbar(cax)\n",
    "    ticks = np.arange(0,9,1)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_xticklabels(columns, rotation = 90)\n",
    "    ax.set_yticklabels(columns)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "# creates correlation matrix\n",
    "create_correlation_matrix(correlation_df, new_cols[2:])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KNN means\n",
    "from sklearn import neighbors\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#features list\n",
    "feature_columns = ['social_housing_count', 'drug_offences', 'rental_housing',\n",
    "                    'born_overseas', 'unemployment_rate', 'year_12_Education', 'support', 'Population_2015']\n",
    "# grab features\n",
    "feature_data = df1[feature_columns]\n",
    "\n",
    "#class label for testing\n",
    "class_label = df1['homeless_count']\n",
    "\n",
    "def create_boxplot(dataframe, column):\n",
    "    \"\"\"\n",
    "    Outputs boxplot of homelessness\n",
    "    \n",
    "    Args:\n",
    "        dataframe: dataframe which we are interested in\n",
    "        column: which column to create plot for\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    data = {'Homelessness per LGA': dataframe[column]}\n",
    "    \n",
    "    df = pd.DataFrame(data=data)\n",
    "    \n",
    "    df.boxplot(column = 'Homelessness per LGA')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "# boxplot of homeless distribution\n",
    "plt.title('Homelessness distribution')\n",
    "plt.ylabel('Homelessness counts per 1000 population')    \n",
    "create_boxplot(df1, 'homeless_count')\n",
    "\n",
    "# information concerning boxplot, such as median and mean\n",
    "print(class_label.describe())\n",
    "\n",
    "# Chose these threshholds\n",
    "threshold1 = 2.1\n",
    "threshold2 = 3.5\n",
    "\n",
    "def create_label_map(class_label, threshold1, threshold2):\n",
    "    \"\"\"\n",
    "    Maps values of homeless count to a class label\n",
    "    \n",
    "    Args:\n",
    "        class_label: A pandas series\n",
    "    Returns:\n",
    "        mapped label data\n",
    "    \"\"\"\n",
    "    labels = [i for i in class_label]\n",
    "    for i, e in enumerate(class_label):\n",
    "        if e <= threshold1:\n",
    "            labels[i] = 1\n",
    "        if threshold1 < e <= threshold2:\n",
    "            labels[i] = 2\n",
    "        if e > threshold2:\n",
    "            labels[i] = 3\n",
    "    return labels\n",
    "\n",
    "#CODE FROM HERE:\n",
    "# Taken from workshop 8, and slightly modified to suit my task\n",
    "\n",
    "# Create new class_label\n",
    "class_label = pd.Series(create_label_map(class_label, threshold1, threshold2))\n",
    "\n",
    "# select 66% of instances to be training data, and the rest 33% will be testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_data, class_label, train_size=0.66, random_state=42)\n",
    "\n",
    "# normalise the data to have 0 mean and unit variance using the library functions.  This will help for later\n",
    "# computation of distances between instances\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "\n",
    "# looking at knn with k = 7\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# predictions for each instance,using the class label. Comparison between prediction and actual class label\n",
    "y_pred=knn.predict(X_test)\n",
    "\n",
    "# overall accuracy\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# possible ranges of k\n",
    "ks = range(1, len(feature_columns)+1)\n",
    "\n",
    "# plotting optimal k values\n",
    "def optimal_k(ks, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Finds optimal k values.\n",
    "    Code taken from Workshop 8 solutions\n",
    "    \n",
    "    Args:\n",
    "        ks: possible k values\n",
    "        X_train: x training data\n",
    "        y_train: y training data\n",
    "        X_test: x testing data\n",
    "        y_test: y testing data\n",
    "    Returns:\n",
    "        list of accuracies\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "    for k in ks:\n",
    "        knn = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train) \n",
    "        y_pred = knn.predict(X_test)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    return accuracies\n",
    "\n",
    "accuracies = optimal_k(ks, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# k versus prediction accuracy\n",
    "plt.scatter(ks, accuracies)\n",
    "plt.title('k versus Prediction accuracy')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Predition Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# decision tree\n",
    "dt = DecisionTreeClassifier(criterion=\"entropy\",random_state=1, max_depth=3)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Another prediction, for each of the test instances, a prediction for a class label is made.Compare this between the real\n",
    "# class label.\n",
    "y_pred=dt.predict(X_test)\n",
    "\n",
    "# overall accuracy\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# decision tree classification accuracy as the size of training set, which is varied between 10% and 90%\n",
    "# demonstrates which training size is most accurate\n",
    "splits = range(10,95,5)\n",
    "\n",
    "def optimal_training_size(feature_data, class_label):\n",
    "    \"\"\"\n",
    "    returns list of training sizes against prediction accuracies\n",
    "    Code taken from Workshop 8 solutions\n",
    "    \n",
    "    Args:\n",
    "        feature_data: the relevent features to be compared with\n",
    "        class_label: the label of the data\n",
    "    Returns \n",
    "        list of training sizes\n",
    "    \"\"\"\n",
    "    accuracies=[]\n",
    "    for split in splits:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(feature_data, class_label, train_size=split/100, random_state=42)\n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        X_train=scaler.transform(X_train)\n",
    "        X_test=scaler.transform(X_test)\n",
    "        dt.fit(X_train, y_train)\n",
    "        y_pred=dt.predict(X_test)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    return accuracies\n",
    "    \n",
    "# scatter plot of training sizes\n",
    "accuracies = optimal_training_size(feature_data, class_label)\n",
    "plt.scatter(splits, accuracies)\n",
    "plt.title('Training size versus prediction accuracy')\n",
    "plt.xlabel('Training size')\n",
    "plt.ylabel('Prediction Accuracy')\n",
    "plt.show()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
